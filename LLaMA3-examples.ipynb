{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3154e97f-09b2-48fb-99d9-f4d27da0ab85",
   "metadata": {},
   "source": [
    "### Download the model files\n",
    "We only need to run these cells once, to download the files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02d10d98-0fcf-44ce-a516-0f6f28e949af",
   "metadata": {},
   "source": [
    "# Import the `notebook_login` function from the `huggingface_hub` module.\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Execute the `notebook_login` function to log in to the Hugging Face Hub.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38a049b0-50c1-4933-bae6-3ffd0fc2664d",
   "metadata": {},
   "source": [
    "# Import hugging face files \n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Repo info\n",
    "repo_id = \"meta-llama/Llama-3.1-8B\"\n",
    "subfolder=\"original\"\n",
    "\n",
    "# Files to download\n",
    "filenames = [\"params.json\", \"tokenizer.model\", \"consolidated.00.pth\"]\n",
    "\n",
    "# Where to save files\n",
    "save_directory = \"llama-3.1-8B\"\n",
    "\n",
    "# Download each file\n",
    "for filename in filenames:\n",
    "    hf_hub_download(\n",
    "        repo_id=repo_id,       # Repository ID\n",
    "        filename=filename,     # Name of the file to download\n",
    "        subfolder=subfolder,   # Subfolder within the repository\n",
    "        local_dir=save_directory  # Directory to save the downloaded file\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac99a94-87be-466d-9dda-9016927b3d41",
   "metadata": {},
   "source": [
    "### Import the libraries we need for tiktoken, PyTorch and json handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffb9e01-1c38-42a7-90c3-1ad89e27dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation library\n",
    "import tiktoken\n",
    "\n",
    "# Byte Pair Encoding function\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "# PyTorch library\n",
    "import torch\n",
    "\n",
    "# JSON \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dd596-cb7a-45e7-aca4-398cd17d1c3c",
   "metadata": {},
   "source": [
    "### Load the model from the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5880478-7f7e-44c1-ab7e-5d0778d4d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "model_directory = \"llama-3.1-8B/original/\"\n",
    "tokenizer_model = load_tiktoken_bpe(model_directory + \"tokenizer.model\") \n",
    "\n",
    "# Load PyTorch model of Llama-3.1-8B\n",
    "model = torch.load(model_directory + \"consolidated.00.pth\",  map_location=torch.device('cpu'), weights_only=True)\n",
    "\n",
    "with open(model_directory + \"params.json\",\"r\") as f:\n",
    "          config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65957759-924a-495e-9688-7678018288f9",
   "metadata": {},
   "source": [
    "### Store model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7bc868-7bc3-4ef3-9bdc-e35c5fbec142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension\n",
    "dim = config[\"dim\"]\n",
    "\n",
    "# Layers\n",
    "n_layers = config[\"n_layers\"]\n",
    "\n",
    "# Heads\n",
    "n_heads = config[\"n_heads\"]\n",
    "\n",
    "# KV_heads\n",
    "n_kv_heads = config[\"n_kv_heads\"]\n",
    "\n",
    "# Vocabulary\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "\n",
    "# Multiple\n",
    "multiple_of = config[\"multiple_of\"]\n",
    "\n",
    "# Multiplier\n",
    "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
    "\n",
    "# Epsilon\n",
    "norm_eps = config[\"norm_eps\"]\n",
    "\n",
    "# RoPE\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce5c66-1654-4900-8641-dfb8611d2b5c",
   "metadata": {},
   "source": [
    "### Function to normalise our input vector using RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d5dc6b-9039-4703-9b88-a9e54819c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating RMSNorm\n",
    "def rms_norm(tensor, norm_weights):\n",
    "    \n",
    "    # Calculate the mean of the square of tensor values along the last dimension\n",
    "    squared_mean = tensor.pow(2).mean(-1, keepdim=True)\n",
    "    \n",
    "    # Add a small value to avoid division by zero\n",
    "    normalized = torch.rsqrt(squared_mean + norm_eps) # note that rsqrt gives the inverse (reciprocal) of the square root\n",
    "    \n",
    "    # Multiply normalized tensor by the provided normalization weights\n",
    "    return (tensor * normalized) * norm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb3b67-44aa-429d-9cb3-7aca0ecb60e2",
   "metadata": {},
   "source": [
    "### Function to transform tokens into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a011cb0-1e8d-41cb-8a4b-49c8ea8b886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given our input tokens tokens (words, parts of words etc), transform these into vector embeddings for each token\n",
    "def tranform_token_sequence_to_embeddings(tokens):\n",
    "\n",
    "    # Define embedding layer with vocab size and embedding dimension\n",
    "    embedding_layer = torch.nn.Embedding(vocab_size, dim)\n",
    "    \n",
    "    # Copy pre-trained token embeddings to the embedding layer\n",
    "    embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
    "    \n",
    "    # Get token embeddings for given tokens, converting to torch.bfloat16 format\n",
    "    token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)\n",
    "    \n",
    "    return token_embeddings_unnormalized "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96cde9-7c16-4de4-97dc-0b39c180c9e5",
   "metadata": {},
   "source": [
    "### Transformer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60130364-0a18-4516-8cd8-41a289e67bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_transformer(embedding, input_seq_length):\n",
    "\n",
    "    # Get freqs_cis\n",
    "    zero_to_one_split_into_64_parts = torch.tensor(range(64))/64\n",
    "    freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "    freqs_for_each_token = torch.outer(torch.arange(input_seq_length), freqs)\n",
    "    # Calculate complex numbers from frequencies_for_each_token using polar coordinates\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
    "    \n",
    "    # Iterate through each layer\n",
    "    for layer in range(n_layers):\n",
    "        # Initialize list to store QKV attentions for each head\n",
    "        qkv_attention_store = []\n",
    "        \n",
    "        # Normalize the final embedding using root mean square normalization and weights from the current layer\n",
    "        layer_embedding_norm = rms_norm(embedding, model[f\"layers.{layer}.attention_norm.weight\"])\n",
    "        \n",
    "        # Retrieve query, key, value, and output weights for the attention mechanism of the current layer\n",
    "        q_layer = model[f\"layers.{layer}.attention.wq.weight\"]\n",
    "        q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)\n",
    "        k_layer = model[f\"layers.{layer}.attention.wk.weight\"]\n",
    "        k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
    "        v_layer = model[f\"layers.{layer}.attention.wv.weight\"]\n",
    "        v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
    "        w_layer = model[f\"layers.{layer}.attention.wo.weight\"]\n",
    "        \n",
    "        # Iterate through each head\n",
    "        for head in range(n_heads):\n",
    "            # Extract query, key, and value weights for the current head\n",
    "            q_layer_head = q_layer[head]\n",
    "            k_layer_head = k_layer[head//4]  # Key weights are shared across 4 heads\n",
    "            v_layer_head = v_layer[head//4]  # Value weights are shared across 4 heads\n",
    "            \n",
    "            # Calculate query per token by matrix multiplication\n",
    "            q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)\n",
    "            \n",
    "            # Calculate key per token by matrix multiplication\n",
    "            k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)\n",
    "            \n",
    "            # Calculate value per token by matrix multiplication\n",
    "            v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)\n",
    "            \n",
    "            # Split query per token into pairs and rotate them\n",
    "            q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "            q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "            q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)\n",
    "            q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "            \n",
    "            # Split key per token into pairs and rotate them\n",
    "            k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "            k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "            k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "            k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "            \n",
    "            # Calculate query-key dot products per token\n",
    "            qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
    "            \n",
    "            # Create a mask tensor filled with negative infinity values\n",
    "            mask = torch.full((len(embedding), len(embedding)), float(\"-inf\"))\n",
    "            # Set upper triangular part of the mask tensor to negative infinity\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            # Add the mask to the query-key dot products per token\n",
    "            qk_per_token_after_masking = qk_per_token + mask\n",
    "            \n",
    "            # Apply softmax along the second dimension after masking\n",
    "            qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "            \n",
    "            # Calculate QKV attention by matrix multiplication\n",
    "            qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "            \n",
    "            # Store QKV attention for the current head\n",
    "            qkv_attention_store.append(qkv_attention)\n",
    "        \n",
    "        # Concatenate QKV attentions from all heads along the last dimension\n",
    "        stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
    "        \n",
    "        # Calculate embedding delta by matrix multiplication with the output weight\n",
    "        embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)\n",
    "        \n",
    "        # Add the embedding delta to the current embedding to get the edited embedding\n",
    "        embedding_after_edit = embedding + embedding_delta\n",
    "        \n",
    "        # Normalize the edited embedding using root mean square normalization and weights from the current layer\n",
    "        embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f\"layers.{layer}.ffn_norm.weight\"])\n",
    "        \n",
    "        # Retrieve weights for the feedforward layer\n",
    "        w1 = model[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
    "        w2 = model[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
    "        w3 = model[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
    "        \n",
    "        # Perform operations for the feedforward layer\n",
    "        output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "        \n",
    "        # Update the final embedding with the edited embedding plus the output from the feedforward layer\n",
    "        embedding = embedding_after_edit + output_after_feedforward\n",
    "    \n",
    "    return embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545254e-15e9-4778-8c09-41b5e9338185",
   "metadata": {},
   "source": [
    "### Create a tokenizer to tokenize our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67ed8fed-3dd5-4c22-a08a-1f2db48700af",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    \"<|begin_of_text|>\",  # Marks the beginning of a text sequence.\n",
    "    \"<|end_of_text|>\",  # Marks the end of a text sequence.\n",
    "    \"<|reserved_special_token_0|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_1|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_2|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_3|>\",  # Reserved for future use.\n",
    "    \"<|start_header_id|>\",  # Indicates the start of a header ID.\n",
    "    \"<|end_header_id|>\",  # Indicates the end of a header ID.\n",
    "    \"<|reserved_special_token_4|>\",  # Reserved for future use.\n",
    "    \"<|eot_id|>\",  # Marks the end of a turn (in a conversational context).\n",
    "] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]  # A large set of tokens reserved for future use.\n",
    "\n",
    "# Set a regex for breaking inpout into tokens\n",
    "tokenize_breaker = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n",
    "\n",
    "# Initialize tokenizer with specified parameters\n",
    "tokenizer = tiktoken.Encoding(\n",
    "\n",
    "    # Name of encoding\n",
    "    name = \"tokenizer.model\",\n",
    "\n",
    "    # Define tokenization pattern string\n",
    "    pat_str = tokenize_breaker,\n",
    "\n",
    "    # Assign BPE mergeable ranks from tokenizer_model of LLaMA-3\n",
    "    mergeable_ranks = tokenizer_model,\n",
    "\n",
    "    # Set special tokens with indices\n",
    "    special_tokens={token: len(tokenizer_model) + i for i, token in enumerate(special_tokens)},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959b1ce-6712-44d5-88ad-d0e720ded106",
   "metadata": {},
   "source": [
    "### Function that takes an input prompt as a lit of tokens and returns the predicted next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75a0d68a-271d-4cd2-b863-78ccc87a153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(tokens):\n",
    "\n",
    "    # Convert the list of tokens into a PyTorch tensor\n",
    "    tokens = torch.tensor(tokens)\n",
    "    input_seq_length = len(tokens)\n",
    "\n",
    "    # Transform prompt to embeddings\n",
    "    final_embedding = tranform_token_sequence_to_embeddings(tokens)\n",
    "\n",
    "    # Run through LLama\n",
    "    final_embedding = llama3_transformer(final_embedding, input_seq_length)\n",
    "\n",
    "    # Normalize the final embedding using root mean square normalization and provided weights\n",
    "    final_embedding = rms_norm(final_embedding, model[\"norm.weight\"])\n",
    "\n",
    "    # Calculate logits by matrix multiplication between the final embedding and the transpose of the output weight tensor\n",
    "    logits = torch.matmul(final_embedding[-1], model[\"output.weight\"].T)\n",
    "\n",
    "    # Find the index of the maximum value along the last dimension to determine the next token\n",
    "    next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return next_token.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85db6590-9050-408c-a046-70d0019ce6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt:  <|begin_of_text|>To paraphrase the opening lines of Anna Karenina: \n",
      "Input prompt as tokens:  [128000, 1271, 63330, 10857, 279, 8736, 5238, 315, 24101, 35745, 2259, 25, 220]\n",
      " \n",
      "All\n",
      " happy\n",
      " families\n",
      " are\n",
      " alike\n",
      ";\n",
      " each\n",
      " unhappy\n",
      " family\n",
      " is\n",
      " unhappy\n",
      " in\n",
      " its\n",
      " own\n",
      "Output:  <|begin_of_text|>To paraphrase the opening lines of Anna Karenina:  All happy families are alike; each unhappy family is unhappy in its own\n",
      "Output as tokens:  [128000, 1271, 63330, 10857, 279, 8736, 5238, 315, 24101, 35745, 2259, 25, 220, 4194, 2460, 6380, 8689, 527, 27083, 26, 1855, 43251, 3070, 374, 43251, 304, 1202, 1866]\n"
     ]
    }
   ],
   "source": [
    "# input prompt\n",
    "prompt = \"To paraphrase the opening lines of Anna Karenina: \"\n",
    "\n",
    "# Encode the prompt using the tokenizer and prepend a <|begin_of_text|> token (128000)\n",
    "prompt_tokens = [128000] + tokenizer.encode(prompt)\n",
    "\n",
    "print(\"Input prompt: \", tokenizer.decode(prompt_tokens))\n",
    "print(\"Input prompt as tokens: \", prompt_tokens)\n",
    "\n",
    "# Generate the next tokens\n",
    "tokens_to_generate = 15\n",
    "for n in range(0,tokens_to_generate):\n",
    "    # Call function to predict next token\n",
    "    next_token = generate_next_token(prompt_tokens)\n",
    "    # Concatenate predicted token onto end of prompt to create next prompt\n",
    "    prompt_tokens = prompt_tokens + [next_token]\n",
    "    print(tokenizer.decode([next_token]))\n",
    "\n",
    "# Decode tokens back to text\n",
    "prompt = tokenizer.decode(prompt_tokens)\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(prompt_tokens))\n",
    "print(\"Output as tokens: \", prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa7f2c-e967-46bb-b1d0-a8c2ca5c424b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
