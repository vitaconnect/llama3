{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f412f014-1760-4477-9dca-84125a55d1c0",
   "metadata": {},
   "source": [
    "# Building LLaMA3.1 from scratch\n",
    "Following https://levelup.gitconnected.com/building-llama-3-from-scratch-with-python-e0cf4dbbc306"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cdb11-9f02-42cb-ba66-4692b6baaa40",
   "metadata": {},
   "source": [
    "## Part 1 - Get the Llama 3 model files from Hugging Face and explore them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9b06e-2220-4e25-8364-636b08ff4ad9",
   "metadata": {},
   "source": [
    "### Download the model files\n",
    "We only need to run these cells once, to download the files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0dc733b-41e0-4c28-8f14-15fd0e99d743",
   "metadata": {},
   "source": [
    "# Import the `notebook_login` function from the `huggingface_hub` module.\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Execute the `notebook_login` function to log in to the Hugging Face Hub.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "527aa889-f4ea-4f82-b12c-60a9df8ef877",
   "metadata": {},
   "source": [
    "# Import hugging face files \n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Repo info\n",
    "repo_id = \"meta-llama/Llama-3.1-8B\"\n",
    "subfolder=\"original\"\n",
    "\n",
    "# Files to download\n",
    "filenames = [\"params.json\", \"tokenizer.model\", \"consolidated.00.pth\"]\n",
    "\n",
    "# Where to save files\n",
    "save_directory = \"llama-3.1-8B\"\n",
    "\n",
    "# Download each file\n",
    "for filename in filenames:\n",
    "    hf_hub_download(\n",
    "        repo_id=repo_id,       # Repository ID\n",
    "        filename=filename,     # Name of the file to download\n",
    "        subfolder=subfolder,   # Subfolder within the repository\n",
    "        local_dir=save_directory  # Directory to save the downloaded file\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac99a94-87be-466d-9dda-9016927b3d41",
   "metadata": {},
   "source": [
    "### Import the libraries we need for tiktoken, PyTorch and json handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9ffb9e01-1c38-42a7-90c3-1ad89e27dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Tokenisation library\n",
    "import tiktoken\n",
    "\n",
    "# Byte Pair Encoding function\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "# PyTorch library\n",
    "import torch\n",
    "\n",
    "# JSON \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dd596-cb7a-45e7-aca4-398cd17d1c3c",
   "metadata": {},
   "source": [
    "### Load and inspect the tokenizer model (tokenizer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d5880478-7f7e-44c1-ab7e-5d0778d4d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer model\n",
    "model_directory = \"llama-3.1-8B/original/\"\n",
    "tokenizer_model = load_tiktoken_bpe(model_directory + \"tokenizer.model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ddab10f0-16f9-4e27-a8ec-7473fdf00f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tokens are in the tokenizer model?  12800\n",
    "len(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1b1f2016-f19d-4c6e-a7f8-ab13bd14131b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What type of object is it? dict\n",
    "type(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3303fc64-6edd-4a8d-8500-abdf43585830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b' Web': 5000,\n",
       " b'Des': 5001,\n",
       " b'BC': 5002,\n",
       " b'ancial': 5003,\n",
       " b'Route': 5004,\n",
       " b'Dec': 5005,\n",
       " b'ferences': 5006,\n",
       " b' purch': 5007,\n",
       " b' Model': 5008,\n",
       " b'ctor': 5009}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 10 items of the model\n",
    "dict(list(tokenizer_model.items())[5000:5010])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea295f8-10e8-4488-a0f9-76edccade429",
   "metadata": {},
   "source": [
    "### Load and inspect the model's learned weights (consolidated.00.pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ed5f029b-3ea4-4e4a-988c-4eb6159d50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PyTorch model of Llama-3.1-8B\n",
    "\n",
    "#   map_location=torch.device('cpu'): this tells torch I'm using CPU rather than CUDA compatible GPU\n",
    "#   weights_only=True: this prevents a security warning\n",
    "model = torch.load(model_directory + \"consolidated.00.pth\",  map_location=torch.device('cpu'), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4834536b-9b82-44dc-993a-b6d74c6f48e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(model)= 291\n"
     ]
    }
   ],
   "source": [
    "print(\"len(model)=\",len(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "94602705-7ec1-463d-914b-aa18f9b52b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model keys are of type  <class 'str'>\n",
      "model values are of type  <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(\"model keys are of type \",type(next(iter(model.keys()), None)))\n",
    "print(\"model values are of type \",type(next(iter(model.values()), None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "72358754-fb3e-4625-8f19-71268a6d8d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok_embeddings.weight',\n",
       " 'layers.0.attention.wq.weight',\n",
       " 'layers.0.attention.wk.weight',\n",
       " 'layers.0.attention.wv.weight',\n",
       " 'layers.0.attention.wo.weight',\n",
       " 'layers.0.feed_forward.w1.weight',\n",
       " 'layers.0.feed_forward.w3.weight',\n",
       " 'layers.0.feed_forward.w2.weight',\n",
       " 'layers.0.attention_norm.weight',\n",
       " 'layers.0.ffn_norm.weight',\n",
       " 'layers.1.attention.wq.weight',\n",
       " 'layers.1.attention.wk.weight',\n",
       " 'layers.1.attention.wv.weight',\n",
       " 'layers.1.attention.wo.weight',\n",
       " 'layers.1.feed_forward.w1.weight',\n",
       " 'layers.1.feed_forward.w3.weight',\n",
       " 'layers.1.feed_forward.w2.weight',\n",
       " 'layers.1.attention_norm.weight',\n",
       " 'layers.1.ffn_norm.weight',\n",
       " 'layers.2.attention.wq.weight']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at some of the keys of the model to see the architecture of the LLM\n",
    "list(model.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "63c091be-0ab0-43e9-9fae-e3e8809493d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layers.31.attention.wq.weight',\n",
       " 'layers.31.attention.wk.weight',\n",
       " 'layers.31.attention.wv.weight',\n",
       " 'layers.31.attention.wo.weight',\n",
       " 'layers.31.feed_forward.w1.weight',\n",
       " 'layers.31.feed_forward.w3.weight',\n",
       " 'layers.31.feed_forward.w2.weight',\n",
       " 'layers.31.attention_norm.weight',\n",
       " 'layers.31.ffn_norm.weight',\n",
       " 'norm.weight',\n",
       " 'output.weight']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.keys())[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c56ef7dc-6438-4747-9752-20a49b3bcce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok_embeddings.weight': tensor([[ 1.2436e-03,  5.6763e-03, -3.2501e-03,  ...,  4.0588e-03,\n",
       "          -2.6245e-03, -6.8283e-04],\n",
       "         [-3.4332e-03,  1.3351e-03, -1.6556e-03,  ...,  9.4604e-04,\n",
       "          -1.8616e-03, -2.1515e-03],\n",
       "         [ 6.7520e-04, -1.6113e-02,  2.5635e-03,  ...,  4.3030e-03,\n",
       "           7.8735e-03,  4.8523e-03],\n",
       "         ...,\n",
       "         [ 2.2230e-23,  3.9291e-24,  2.1713e-23,  ...,  6.4106e-23,\n",
       "          -2.6625e-24, -2.3678e-23],\n",
       "         [ 2.2954e-23, -2.2230e-24, -2.2334e-23,  ...,  2.8124e-23,\n",
       "           8.7371e-24, -3.7223e-23],\n",
       "         [-8.8922e-23, -7.6101e-23,  6.5140e-24,  ...,  5.9195e-24,\n",
       "          -6.4934e-23, -2.7271e-24]], dtype=torch.bfloat16),\n",
       " 'layers.0.attention.wq.weight': tensor([[ 0.0053, -0.0291, -0.0058,  ...,  0.0095, -0.0420, -0.0272],\n",
       "         [ 0.0284,  0.0008, -0.0093,  ..., -0.0092, -0.0078,  0.0048],\n",
       "         [-0.0142, -0.0679, -0.0049,  ..., -0.0142, -0.0498,  0.0192],\n",
       "         ...,\n",
       "         [-0.0035, -0.0101,  0.0459,  ...,  0.0049, -0.0011,  0.0011],\n",
       "         [ 0.0006,  0.0309, -0.0698,  ..., -0.0028, -0.0002, -0.0019],\n",
       "         [-0.0018, -0.0153,  0.0347,  ...,  0.0110,  0.0004,  0.0044]],\n",
       "        dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's lool at one entry\n",
    "dict(list(model.items())[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296796a-fa3d-48a3-9df8-56321b2d977c",
   "metadata": {},
   "source": [
    "### Load and inspect model hyperparameters (params.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bfa310b8-f9f8-4a4f-a000-5210a9981487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 4096, 'ffn_dim_multiplier': 1.3, 'multiple_of': 1024, 'n_heads': 32, 'n_kv_heads': 8, 'n_layers': 32, 'norm_eps': 1e-05, 'rope_theta': 500000.0, 'use_scaled_rope': True, 'vocab_size': 128256}\n"
     ]
    }
   ],
   "source": [
    "with open(model_directory + \"params.json\",\"r\") as f:\n",
    "          config = json.load(f)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "629105c0-44d6-4ddc-9551-4cac2e335378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77241abb-f921-49ba-bd03-129cef2cec35",
   "metadata": {},
   "source": [
    "## Part 2 - Build our Llama-3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65957759-924a-495e-9688-7678018288f9",
   "metadata": {},
   "source": [
    "### Store model hyperparameters for use later in building our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7f7bc868-7bc3-4ef3-9bdc-e35c5fbec142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension\n",
    "dim = config[\"dim\"]\n",
    "\n",
    "# Layers\n",
    "n_layers = config[\"n_layers\"]\n",
    "\n",
    "# Heads\n",
    "n_heads = config[\"n_heads\"]\n",
    "\n",
    "# KV_heads\n",
    "n_kv_heads = config[\"n_kv_heads\"]\n",
    "\n",
    "# Vocabulary\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "\n",
    "# Multiple\n",
    "multiple_of = config[\"multiple_of\"]\n",
    "\n",
    "# Multiplier\n",
    "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
    "\n",
    "# Epsilon\n",
    "norm_eps = config[\"norm_eps\"]\n",
    "\n",
    "# RoPE\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46c95d-9f9c-4889-b6f0-6c2f4ae99dd1",
   "metadata": {},
   "source": [
    "### Create a tokenizer to tokenize our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "67ed8fed-3dd5-4c22-a08a-1f2db48700af",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    \"<|begin_of_text|>\",  # Marks the beginning of a text sequence.\n",
    "    \"<|end_of_text|>\",  # Marks the end of a text sequence.\n",
    "    \"<|reserved_special_token_0|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_1|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_2|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_3|>\",  # Reserved for future use.\n",
    "    \"<|start_header_id|>\",  # Indicates the start of a header ID.\n",
    "    \"<|end_header_id|>\",  # Indicates the end of a header ID.\n",
    "    \"<|reserved_special_token_4|>\",  # Reserved for future use.\n",
    "    \"<|eot_id|>\",  # Marks the end of a turn (in a conversational context).\n",
    "] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]  # A large set of tokens reserved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "64c44820-a5d9-4947-ac2d-eda6a660ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a regex for breaking inpout into tokens\n",
    "tokenize_breaker = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e1acae2e-8f38-40ed-8ceb-addf097c1c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", ' happy', '!', ' It', \"'s\", ' ', '100', '%', ' true', '.']\n"
     ]
    }
   ],
   "source": [
    "# Here's how the tokenize_breaker regex works on an example sentence\n",
    "import re\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"I'm happy! It's 100% true.\"\n",
    "\n",
    "# Since the regex pattern uses Unicode property escapes like \\p{L}, we will use the 'regex' module (not the built-in 're' module) in Python,\n",
    "# which supports Unicode property escapes directly.\n",
    "import regex as re\n",
    "\n",
    "# Tokenize the sentence using the regex pattern\n",
    "tokens = re.findall(tokenize_breaker, sentence)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2acccb85-806a-4fa8-81c7-f389510a0691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello world, I'm 10% pleased to see you!\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize tokenizer with specified parameters\n",
    "tokenizer = tiktoken.Encoding(\n",
    "\n",
    "    # Name of encoding\n",
    "    name = \"tokenizer.model\",\n",
    "\n",
    "    # Define tokenization pattern string\n",
    "    pat_str = tokenize_breaker,\n",
    "\n",
    "    # Assign BPE mergeable ranks from tokenizer_model of LLaMA-3\n",
    "    mergeable_ranks = tokenizer_model,\n",
    "\n",
    "    # Set special tokens with indices\n",
    "    special_tokens={token: len(tokenizer_model) + i for i, token in enumerate(special_tokens)},\n",
    ")\n",
    "\n",
    "# Encode \"hello world!\" and decode tokens to string\n",
    "tokenizer.decode(tokenizer.encode(\"hello world, I'm 10% pleased to see you!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dae9a958-0d9d-4335-9403-3e0de32a43ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339, 1917, 11, 358, 2846, 220, 605, 4, 18949, 311, 1518, 499, 0]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello world, I'm 10% pleased to see you!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "56d9b88f-86b0-43ab-83e6-c3a7320941cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|begin_of_text|>', 128000),\n",
       " ('<|end_of_text|>', 128001),\n",
       " ('<|reserved_special_token_0|>', 128002),\n",
       " ('<|reserved_special_token_1|>', 128003),\n",
       " ('<|reserved_special_token_2|>', 128004)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at those special_tokens.  \n",
    "# These are additional tokens we're adding to the \"end\" of tokenizer_model\n",
    "special_tokens={token: len(tokenizer_model) + i for i, token in enumerate(special_tokens)}\n",
    "\n",
    "# Show first 5 entries\n",
    "list(special_tokens.items())[0: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9a2d7707-dd13-439d-81fc-8e8f53934b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 53770, 4848, 8254, 8223, 11, 889, 656, 584]\n",
      "['<|begin_of_text|>', 'five', ' six', ' seven', ' eight', ',', ' who', ' do', ' we']\n"
     ]
    }
   ],
   "source": [
    "# input prompt\n",
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "\n",
    "prompt = \"If I could \"\n",
    "\n",
    "# Encode the prompt using the tokenizer and prepend a special token (128000)\n",
    "tokens = [128000] + tokenizer.encode(prompt)\n",
    "\n",
    "print(tokens)  # Print the encoded tokens\n",
    "\n",
    "# Convert the list of tokens into a PyTorch tensor\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "\n",
    "# Decode each token back into its corresponding string\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]\n",
    "\n",
    "print(prompt_split_as_tokens)  # Print the decoded tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "948772aa-7ff9-46b5-97d8-fd6343b045be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check length of our input vector\n",
    "input_seq_length = len(tokens)\n",
    "input_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f55c3862-d98f-4964-bda8-dae494b54a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "# What are the dimensions of the embedding vector for llama-3.1?\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4d425dd8-5bce-4c97-958a-28bd4ae298ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f41eb84-6371-4823-9971-620d1b43a4bd",
   "metadata": {},
   "source": [
    "### Transform input sequence to embeddings for each word in the sequence\n",
    "We need to transform our input vector (sequence of seventeen words encoded as tokens) from \n",
    "its current dimensions of (17 x 1) to (17 x 4096), to capture the embedding for each of the \n",
    "seventeen words in our input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "51661b06-24b2-4b03-9354-49132e14266b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define embedding layer with vocab size and embedding dimension\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, dim)\n",
    "\n",
    "# Copy pre-trained token embeddings to the embedding layer\n",
    "embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
    "\n",
    "# Get token embeddings for given tokens, converting to torch.bfloat16 format\n",
    "token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)\n",
    "\n",
    "# Print shape of resulting token embeddings\n",
    "token_embeddings_unnormalized.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec1fa3-9b76-4854-9260-795ab5f08279",
   "metadata": {},
   "source": [
    "### Normalise our input vector using RMSNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e7d5dc6b-9039-4703-9b88-a9e54819c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating RMSNorm\n",
    "def rms_norm(tensor, norm_weights):\n",
    "\n",
    "    #print(\"tensor.shape = \", tensor.shape)\n",
    "    #print(\"norm_weights.shape = \", norm_weights.shape)\n",
    "    \n",
    "    # Calculate the mean of the square of tensor values along the last dimension\n",
    "    squared_mean = tensor.pow(2).mean(-1, keepdim=True)\n",
    "    #print(\"squared_mean.shape = \",squared_mean.shape)\n",
    "    #print(\"i.e. for each of the tokens, the mean of the squares of the embedding values for that token\")\n",
    "    #print(\"squared_mean = \",squared_mean)\n",
    "    \n",
    "    # Add a small value to avoid division by zero\n",
    "    normalized = torch.rsqrt(squared_mean + norm_eps) # note that rsqrt gives the inverse (reciprocal) of the square root\n",
    "    #print(\"normalized.shape = \",normalized.shape)\n",
    "    #print(\"i.e. the inverse square root of each squared mean\")\n",
    "    #print(\"normalized = \",normalized)\n",
    "    \n",
    "    # Multiply normalized tensor by the provided normalization weights\n",
    "    return (tensor * normalized) * norm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ef3ed-f98d-48b8-bc30-0788d23a37a9",
   "metadata": {},
   "source": [
    "Use the attention weights from the first layer of our transformer architecture to normalise our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "89784118-1ace-4cfb-9e75-f5eb1b36563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using RMS normalization and provided normalization weights\n",
    "token_embeddings = rms_norm(token_embeddings_unnormalized, \n",
    "                            model[\"layers.0.attention_norm.weight\"])\n",
    "\n",
    "# Print the shape of the resulting token embeddings\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6b49d-9556-46de-8510-0f220809d4e0",
   "metadata": {},
   "source": [
    "## Part 3: Start building our attention heads.  Build the first head for the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "efc7be39-6664-4bed-a8a1-f98b1964d573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of different weights\n",
    "print(\n",
    "    # Query weight shape\n",
    "    model[\"layers.0.attention.wq.weight\"].shape,\n",
    "    \n",
    "    # Key weight shape\n",
    "    model[\"layers.0.attention.wk.weight\"].shape,\n",
    "    \n",
    "    # Value weight shape\n",
    "    model[\"layers.0.attention.wv.weight\"].shape,\n",
    "    \n",
    "    # Output weight shape\n",
    "    model[\"layers.0.attention.wo.weight\"].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad5614-2f7f-4c63-80e4-ac18d3352991",
   "metadata": {},
   "source": [
    "Now remember that each layer (in this case layer zero) has a number of attention heads.  These weights combine the weights for all the heads in this layer.  Let's reshape so that each attention head's wieghts are separate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9d00c6a1-aefc-4d0d-88dc-ff1dc28e292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_dim =  128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 4096])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve query weight for the first layer of attention\n",
    "q_layer0 = model[\"layers.0.attention.wq.weight\"]\n",
    "\n",
    "# Calculate dimension per head\n",
    "head_dim = q_layer0.shape[0] // n_heads\n",
    "print(\"head_dim = \", head_dim)\n",
    "\n",
    "# Reshape query weight to separate heads\n",
    "q_layer0 = q_layer0.view(n_heads, head_dim, dim)\n",
    "\n",
    "# Print the shape of the reshaped query weight tensor\n",
    "q_layer0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "538a48df-bbee-42fc-af8a-d484aca58ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4096])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the query weight for the first head of the first layer of attention\n",
    "q_layer0_head0 = q_layer0[0]\n",
    "\n",
    "# Print the shape of the extracted query weight tensor for the first head\n",
    "q_layer0_head0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c767e-86a0-46b3-b1b7-815c673bcdc0",
   "metadata": {},
   "source": [
    "Multiply the query weights with the token embedding for each token, to get our query vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0469c0c3-c73a-4dcf-8e2f-44cd87b7c893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings.shape =  torch.Size([9, 4096])\n",
      "q_layer0_head0.T.shape =  torch.Size([4096, 128])\n",
      "q_per_token.shape =  torch.Size([9, 128])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication: token embeddings with transpose of query weight for first head\n",
    "print(\"token_embeddings.shape = \", token_embeddings.shape)\n",
    "print(\"q_layer0_head0.T.shape = \", q_layer0_head0.T.shape)\n",
    "\n",
    "q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)\n",
    "\n",
    "# Shape of resulting tensor: queries per token\n",
    "print(\"q_per_token.shape = \",q_per_token.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599a326-74c5-4814-b29a-0a11e5650617",
   "metadata": {},
   "source": [
    "### Implement positional encoding with RoPE\n",
    "We now need to implement positional encoding so that the query vectors encode something about their position in the sequence.  \n",
    "Split the query vectors into pairs and apply rotational angle shift to each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1552c011-3d32-4b2b-930b-24814ce175ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 64, 2])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert queries per token to float and split into pairs\n",
    "q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "\n",
    "# Print the shape of the resulting tensor after splitting into pairs\n",
    "q_per_token_split_into_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "284c500d-79dd-45db-a1ce-2bebf7d0d258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0840, -0.1611,  0.2910,  ...,  0.9219,  0.5000,  0.3457],\n",
       "        [ 0.4473, -0.2129,  0.8164,  ...,  1.3047,  1.5234,  1.4766],\n",
       "        [ 0.9375, -0.0076,  2.1094,  ...,  1.1797,  1.7031,  1.4219],\n",
       "        ...,\n",
       "        [ 0.8867,  0.1729,  2.5781,  ...,  2.8594,  1.6172,  1.1094],\n",
       "        [ 0.9023,  0.2383,  2.6406,  ...,  2.5000,  1.2969,  0.8047],\n",
       "        [ 0.7461,  0.1377,  2.1875,  ...,  2.2344,  1.5391,  1.1016]],\n",
       "       dtype=torch.bfloat16, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9089e455-c6c4-440e-9517-0ce2ab579316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0840, -0.1611],\n",
       "         [ 0.2910, -0.2910],\n",
       "         [ 0.3184, -0.4141],\n",
       "         ...,\n",
       "         [ 0.4980, -0.1328],\n",
       "         [ 0.4336,  0.9219],\n",
       "         [ 0.5000,  0.3457]],\n",
       "\n",
       "        [[ 0.4473, -0.2129],\n",
       "         [ 0.8164, -0.7383],\n",
       "         [ 0.6016, -1.0859],\n",
       "         ...,\n",
       "         [ 1.0312,  0.0830],\n",
       "         [ 0.5898,  1.3047],\n",
       "         [ 1.5234,  1.4766]],\n",
       "\n",
       "        [[ 0.9375, -0.0076],\n",
       "         [ 2.1094, -1.1484],\n",
       "         [ 1.3750, -1.8516],\n",
       "         ...,\n",
       "         [ 1.2344, -0.4648],\n",
       "         [ 0.1094,  1.1797],\n",
       "         [ 1.7031,  1.4219]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8867,  0.1729],\n",
       "         [ 2.5781, -1.1719],\n",
       "         [ 1.7969, -2.5156],\n",
       "         ...,\n",
       "         [ 1.4609, -0.5469],\n",
       "         [ 0.4844,  2.8594],\n",
       "         [ 1.6172,  1.1094]],\n",
       "\n",
       "        [[ 0.9023,  0.2383],\n",
       "         [ 2.6406, -1.1641],\n",
       "         [ 1.5703, -2.2031],\n",
       "         ...,\n",
       "         [ 1.3594, -0.0270],\n",
       "         [ 0.8516,  2.5000],\n",
       "         [ 1.2969,  0.8047]],\n",
       "\n",
       "        [[ 0.7461,  0.1377],\n",
       "         [ 2.1875, -0.8555],\n",
       "         [ 1.5547, -2.0625],\n",
       "         ...,\n",
       "         [ 1.6328, -0.5625],\n",
       "         [ 0.8086,  2.2344],\n",
       "         [ 1.5391,  1.1016]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_per_token_split_into_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "edf615a9-20f3-4464-a7fd-e9f6712e74dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,\n",
       "        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,\n",
       "        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,\n",
       "        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,\n",
       "        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,\n",
       "        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,\n",
       "        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,\n",
       "        0.9844])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate values from 0 to 1 split into 64 parts\n",
    "zero_to_one_split_into_64_parts = torch.tensor(range(64))/64\n",
    "\n",
    "# Print the resulting tensor\n",
    "zero_to_one_split_into_64_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0e017063-6cf6-487a-a2a8-421608227a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(500000.)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rope_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "af464d51-c1ed-45cc-beb0-2e69d7fb2594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,\n",
       "        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,\n",
       "        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,\n",
       "        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,\n",
       "        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,\n",
       "        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,\n",
       "        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,\n",
       "        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,\n",
       "        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,\n",
       "        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,\n",
       "        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "\n",
    "# Display the resulting frequencies\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9912a658-7ba2-400c-a8fd-65a5af0983e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148845202809214"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(500000**0.0156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "80f246fe-7f32-4b99-80f9-4c6c6b764e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_per_token_as_complex_numbers.shape =  torch.Size([9, 64])\n",
      "freqs_for_each_token.shape =  torch.Size([9, 64])\n",
      "freqs_for_each_token =  tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,\n",
      "         2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,\n",
      "         8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,\n",
      "         2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,\n",
      "         7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,\n",
      "         2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,\n",
      "         6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,\n",
      "         1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,\n",
      "         5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,\n",
      "         1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,\n",
      "         4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06],\n",
      "        [2.0000e+00, 1.6292e+00, 1.3272e+00, 1.0812e+00, 8.8073e-01, 7.1746e-01,\n",
      "         5.8446e-01, 4.7611e-01, 3.8785e-01, 3.1595e-01, 2.5737e-01, 2.0966e-01,\n",
      "         1.7079e-01, 1.3913e-01, 1.1334e-01, 9.2328e-02, 7.5212e-02, 6.1269e-02,\n",
      "         4.9911e-02, 4.0658e-02, 3.3121e-02, 2.6981e-02, 2.1979e-02, 1.7905e-02,\n",
      "         1.4585e-02, 1.1881e-02, 9.6788e-03, 7.8846e-03, 6.4229e-03, 5.2322e-03,\n",
      "         4.2622e-03, 3.4721e-03, 2.8284e-03, 2.3041e-03, 1.8769e-03, 1.5290e-03,\n",
      "         1.2455e-03, 1.0146e-03, 8.2655e-04, 6.7332e-04, 5.4850e-04, 4.4681e-04,\n",
      "         3.6398e-04, 2.9651e-04, 2.4154e-04, 1.9676e-04, 1.6029e-04, 1.3057e-04,\n",
      "         1.0637e-04, 8.6648e-05, 7.0585e-05, 5.7499e-05, 4.6840e-05, 3.8157e-05,\n",
      "         3.1083e-05, 2.5321e-05, 2.0627e-05, 1.6803e-05, 1.3688e-05, 1.1150e-05,\n",
      "         9.0833e-06, 7.3994e-06, 6.0277e-06, 4.9103e-06],\n",
      "        [3.0000e+00, 2.4439e+00, 1.9908e+00, 1.6217e+00, 1.3211e+00, 1.0762e+00,\n",
      "         8.7668e-01, 7.1416e-01, 5.8177e-01, 4.7392e-01, 3.8606e-01, 3.1449e-01,\n",
      "         2.5619e-01, 2.0870e-01, 1.7001e-01, 1.3849e-01, 1.1282e-01, 9.1904e-02,\n",
      "         7.4866e-02, 6.0987e-02, 4.9681e-02, 4.0471e-02, 3.2969e-02, 2.6857e-02,\n",
      "         2.1878e-02, 1.7822e-02, 1.4518e-02, 1.1827e-02, 9.6343e-03, 7.8483e-03,\n",
      "         6.3934e-03, 5.2081e-03, 4.2426e-03, 3.4561e-03, 2.8154e-03, 2.2935e-03,\n",
      "         1.8683e-03, 1.5220e-03, 1.2398e-03, 1.0100e-03, 8.2274e-04, 6.7022e-04,\n",
      "         5.4597e-04, 4.4476e-04, 3.6231e-04, 2.9514e-04, 2.4043e-04, 1.9586e-04,\n",
      "         1.5955e-04, 1.2997e-04, 1.0588e-04, 8.6249e-05, 7.0260e-05, 5.7235e-05,\n",
      "         4.6625e-05, 3.7981e-05, 3.0940e-05, 2.5204e-05, 2.0532e-05, 1.6726e-05,\n",
      "         1.3625e-05, 1.1099e-05, 9.0416e-06, 7.3654e-06],\n",
      "        [4.0000e+00, 3.2585e+00, 2.6544e+00, 2.1623e+00, 1.7615e+00, 1.4349e+00,\n",
      "         1.1689e+00, 9.5222e-01, 7.7569e-01, 6.3189e-01, 5.1475e-01, 4.1932e-01,\n",
      "         3.4159e-01, 2.7826e-01, 2.2668e-01, 1.8466e-01, 1.5042e-01, 1.2254e-01,\n",
      "         9.9822e-02, 8.1316e-02, 6.6242e-02, 5.3962e-02, 4.3958e-02, 3.5809e-02,\n",
      "         2.9171e-02, 2.3763e-02, 1.9358e-02, 1.5769e-02, 1.2846e-02, 1.0464e-02,\n",
      "         8.5245e-03, 6.9442e-03, 5.6569e-03, 4.6082e-03, 3.7539e-03, 3.0580e-03,\n",
      "         2.4911e-03, 2.0293e-03, 1.6531e-03, 1.3466e-03, 1.0970e-03, 8.9363e-04,\n",
      "         7.2797e-04, 5.9301e-04, 4.8308e-04, 3.9352e-04, 3.2057e-04, 2.6114e-04,\n",
      "         2.1273e-04, 1.7330e-04, 1.4117e-04, 1.1500e-04, 9.3680e-05, 7.6313e-05,\n",
      "         6.2166e-05, 5.0642e-05, 4.1254e-05, 3.3606e-05, 2.7376e-05, 2.2301e-05,\n",
      "         1.8167e-05, 1.4799e-05, 1.2055e-05, 9.8206e-06],\n",
      "        [5.0000e+00, 4.0731e+00, 3.3180e+00, 2.7029e+00, 2.2018e+00, 1.7937e+00,\n",
      "         1.4611e+00, 1.1903e+00, 9.6961e-01, 7.8986e-01, 6.4344e-01, 5.2415e-01,\n",
      "         4.2699e-01, 3.4783e-01, 2.8335e-01, 2.3082e-01, 1.8803e-01, 1.5317e-01,\n",
      "         1.2478e-01, 1.0165e-01, 8.2802e-02, 6.7452e-02, 5.4948e-02, 4.4761e-02,\n",
      "         3.6463e-02, 2.9704e-02, 2.4197e-02, 1.9711e-02, 1.6057e-02, 1.3080e-02,\n",
      "         1.0656e-02, 8.6802e-03, 7.0711e-03, 5.7602e-03, 4.6924e-03, 3.8225e-03,\n",
      "         3.1139e-03, 2.5366e-03, 2.0664e-03, 1.6833e-03, 1.3712e-03, 1.1170e-03,\n",
      "         9.0996e-04, 7.4127e-04, 6.0385e-04, 4.9191e-04, 4.0071e-04, 3.2643e-04,\n",
      "         2.6591e-04, 2.1662e-04, 1.7646e-04, 1.4375e-04, 1.1710e-04, 9.5392e-05,\n",
      "         7.7708e-05, 6.3302e-05, 5.1567e-05, 4.2007e-05, 3.4220e-05, 2.7876e-05,\n",
      "         2.2708e-05, 1.8499e-05, 1.5069e-05, 1.2276e-05],\n",
      "        [6.0000e+00, 4.8877e+00, 3.9816e+00, 3.2435e+00, 2.6422e+00, 2.1524e+00,\n",
      "         1.7534e+00, 1.4283e+00, 1.1635e+00, 9.4784e-01, 7.7212e-01, 6.2899e-01,\n",
      "         5.1238e-01, 4.1740e-01, 3.4002e-01, 2.7698e-01, 2.2564e-01, 1.8381e-01,\n",
      "         1.4973e-01, 1.2197e-01, 9.9363e-02, 8.0943e-02, 6.5937e-02, 5.3714e-02,\n",
      "         4.3756e-02, 3.5644e-02, 2.9037e-02, 2.3654e-02, 1.9269e-02, 1.5697e-02,\n",
      "         1.2787e-02, 1.0416e-02, 8.4853e-03, 6.9123e-03, 5.6308e-03, 4.5870e-03,\n",
      "         3.7366e-03, 3.0439e-03, 2.4796e-03, 2.0200e-03, 1.6455e-03, 1.3404e-03,\n",
      "         1.0919e-03, 8.8952e-04, 7.2462e-04, 5.9029e-04, 4.8086e-04, 3.9171e-04,\n",
      "         3.1910e-04, 2.5994e-04, 2.1175e-04, 1.7250e-04, 1.4052e-04, 1.1447e-04,\n",
      "         9.3249e-05, 7.5962e-05, 6.1880e-05, 5.0409e-05, 4.1064e-05, 3.3451e-05,\n",
      "         2.7250e-05, 2.2198e-05, 1.8083e-05, 1.4731e-05],\n",
      "        [7.0000e+00, 5.7023e+00, 4.6452e+00, 3.7841e+00, 3.0826e+00, 2.5111e+00,\n",
      "         2.0456e+00, 1.6664e+00, 1.3575e+00, 1.1058e+00, 9.0081e-01, 7.3382e-01,\n",
      "         5.9778e-01, 4.8696e-01, 3.9669e-01, 3.2315e-01, 2.6324e-01, 2.1444e-01,\n",
      "         1.7469e-01, 1.4230e-01, 1.1592e-01, 9.4433e-02, 7.6927e-02, 6.2666e-02,\n",
      "         5.1049e-02, 4.1585e-02, 3.3876e-02, 2.7596e-02, 2.2480e-02, 1.8313e-02,\n",
      "         1.4918e-02, 1.2152e-02, 9.8995e-03, 8.0643e-03, 6.5693e-03, 5.3515e-03,\n",
      "         4.3594e-03, 3.5512e-03, 2.8929e-03, 2.3566e-03, 1.9197e-03, 1.5639e-03,\n",
      "         1.2739e-03, 1.0378e-03, 8.4539e-04, 6.8867e-04, 5.6100e-04, 4.5700e-04,\n",
      "         3.7228e-04, 3.0327e-04, 2.4705e-04, 2.0125e-04, 1.6394e-04, 1.3355e-04,\n",
      "         1.0879e-04, 8.8623e-05, 7.2194e-05, 5.8810e-05, 4.7908e-05, 3.9027e-05,\n",
      "         3.1792e-05, 2.5898e-05, 2.1097e-05, 1.7186e-05],\n",
      "        [8.0000e+00, 6.5169e+00, 5.3088e+00, 4.3246e+00, 3.5229e+00, 2.8698e+00,\n",
      "         2.3378e+00, 1.9044e+00, 1.5514e+00, 1.2638e+00, 1.0295e+00, 8.3865e-01,\n",
      "         6.8318e-01, 5.5653e-01, 4.5336e-01, 3.6931e-01, 3.0085e-01, 2.4508e-01,\n",
      "         1.9964e-01, 1.6263e-01, 1.3248e-01, 1.0792e-01, 8.7916e-02, 7.1618e-02,\n",
      "         5.8341e-02, 4.7526e-02, 3.8715e-02, 3.1538e-02, 2.5692e-02, 2.0929e-02,\n",
      "         1.7049e-02, 1.3888e-02, 1.1314e-02, 9.2163e-03, 7.5078e-03, 6.1160e-03,\n",
      "         4.9822e-03, 4.0586e-03, 3.3062e-03, 2.6933e-03, 2.1940e-03, 1.7873e-03,\n",
      "         1.4559e-03, 1.1860e-03, 9.6616e-04, 7.8705e-04, 6.4114e-04, 5.2229e-04,\n",
      "         4.2546e-04, 3.4659e-04, 2.8234e-04, 2.3000e-04, 1.8736e-04, 1.5263e-04,\n",
      "         1.2433e-04, 1.0128e-04, 8.2507e-05, 6.7212e-05, 5.4752e-05, 4.4602e-05,\n",
      "         3.6333e-05, 2.9598e-05, 2.4111e-05, 1.9641e-05]])\n",
      "q_per_token_as_complex_numbers_rotated.shape =  torch.Size([9, 64])\n"
     ]
    }
   ],
   "source": [
    "# Convert queries per token to complex numbers\n",
    "q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "\n",
    "print(\"q_per_token_as_complex_numbers.shape = \", q_per_token_as_complex_numbers.shape)\n",
    "\n",
    "# Calculate frequencies for each token using outer product of arange(17) and freqs\n",
    "# freqs_for_each_token = torch.outer(torch.arange(17), freqs)\n",
    "freqs_for_each_token = torch.outer(torch.arange(input_seq_length), freqs)\n",
    "print(\"freqs_for_each_token.shape = \", freqs_for_each_token.shape)\n",
    "print(\"freqs_for_each_token = \", freqs_for_each_token)\n",
    "\n",
    "# Calculate complex numbers from frequencies_for_each_token using polar coordinates\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
    "\n",
    "# Rotate complex numbers by frequencies\n",
    "q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis\n",
    "\n",
    "print(\"q_per_token_as_complex_numbers_rotated.shape = \", q_per_token_as_complex_numbers_rotated.shape)\n",
    "# Output: torch.Size([17, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fca62356-56e3-490f-ad77-b84cc72b9b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 64, 2])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert rotated complex numbers back to real numbers\n",
    "q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "q_per_token_split_into_pairs_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d13cb92d-e851-433b-9771-1e848760ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 128])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape rotated token queries to match the original shape\n",
    "q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "q_per_token_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "12fd1a7d-f660-49d4-a4e1-de6fe7426a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0840, -0.1611],\n",
       "         [ 0.2910, -0.2910],\n",
       "         [ 0.3184, -0.4141],\n",
       "         ...,\n",
       "         [ 0.4980, -0.1328],\n",
       "         [ 0.4336,  0.9219],\n",
       "         [ 0.5000,  0.3457]],\n",
       "\n",
       "        [[ 0.4208,  0.2613],\n",
       "         [ 1.0972,  0.0873],\n",
       "         [ 1.1428, -0.4849],\n",
       "         ...,\n",
       "         [ 1.0312,  0.0830],\n",
       "         [ 0.5898,  1.3047],\n",
       "         [ 1.5234,  1.4766]],\n",
       "\n",
       "        [[-0.3833,  0.8556],\n",
       "         [ 1.0233,  2.1728],\n",
       "         [ 2.1285,  0.8878],\n",
       "         ...,\n",
       "         [ 1.2344, -0.4648],\n",
       "         [ 0.1094,  1.1797],\n",
       "         [ 1.7031,  1.4219]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8997, -0.0818],\n",
       "         [-0.7042, -2.7430],\n",
       "         [-3.0726,  0.3410],\n",
       "         ...,\n",
       "         [ 1.4609, -0.5468],\n",
       "         [ 0.4843,  2.8594],\n",
       "         [ 1.6172,  1.1094]],\n",
       "\n",
       "        [[ 0.5237,  0.7725],\n",
       "         [ 1.5688, -2.4222],\n",
       "         [-2.3036, -1.4189],\n",
       "         ...,\n",
       "         [ 1.3594, -0.0269],\n",
       "         [ 0.8515,  2.5000],\n",
       "         [ 1.2969,  0.8047]],\n",
       "\n",
       "        [[-0.2448,  0.7181],\n",
       "         [ 2.3262, -0.3255],\n",
       "         [-0.8332, -2.4447],\n",
       "         ...,\n",
       "         [ 1.6328, -0.5625],\n",
       "         [ 0.8085,  2.2344],\n",
       "         [ 1.5390,  1.1016]]], grad_fn=<ViewAsRealBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_per_token_split_into_pairs_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "021d9883-7498-41db-ad5d-4e511b415b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0840, -0.1611,  0.2910,  ...,  0.9219,  0.5000,  0.3457],\n",
       "        [ 0.4208,  0.2613,  1.0972,  ...,  1.3047,  1.5234,  1.4766],\n",
       "        [-0.3833,  0.8556,  1.0233,  ...,  1.1797,  1.7031,  1.4219],\n",
       "        ...,\n",
       "        [ 0.8997, -0.0818, -0.7042,  ...,  2.8594,  1.6172,  1.1094],\n",
       "        [ 0.5237,  0.7725,  1.5688,  ...,  2.5000,  1.2969,  0.8047],\n",
       "        [-0.2448,  0.7181,  2.3262,  ...,  2.2344,  1.5390,  1.1016]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_per_token_rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ca52a-0910-4522-bd1f-25d81a86e43b",
   "metadata": {},
   "source": [
    "### Now do the same for Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b3d1866d-9f53-4c13-ab35-4c49659b153b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 128])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the weight tensor for the attention mechanism's key in the first layer of the model\n",
    "k_layer0 = model[\"layers.0.attention.wk.weight\"]\n",
    "\n",
    "# Reshape key weight for the first layer of attention to separate heads\n",
    "k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)\n",
    "\n",
    "# Print the shape of the reshaped key weight tensor\n",
    "k_layer0.shape  # Output: torch.Size([8, 128, 4096])\n",
    "\n",
    "# Extract the key weight for the first head of the first layer of attention\n",
    "k_layer0_head0 = k_layer0[0]\n",
    "\n",
    "# Print the shape of the extracted key weight tensor for the first head\n",
    "k_layer0_head0.shape  # Output: torch.Size([128, 4096])\n",
    "\n",
    "# Calculate key per token by matrix multiplication\n",
    "k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)\n",
    "\n",
    "# Print the shape of the resulting tensor representing keys per token\n",
    "k_per_token.shape  # Output: torch.Size([17, 128])\n",
    "\n",
    "# Split key per token into pairs and convert to float\n",
    "k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "\n",
    "# Print the shape of the resulting tensor after splitting into pairs\n",
    "k_per_token_split_into_pairs.shape  # Output: torch.Size([17, 64, 2])\n",
    "\n",
    "# Convert key per token to complex numbers\n",
    "k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "\n",
    "# Print the shape of the resulting tensor representing key per token as complex numbers\n",
    "k_per_token_as_complex_numbers.shape  # Output: torch.Size([17, 64])\n",
    "\n",
    "# Rotate complex key per token by frequencies\n",
    "k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "\n",
    "# Print the shape of the rotated complex key per token\n",
    "k_per_token_split_into_pairs_rotated.shape  # Output: torch.Size([17, 64, 2])\n",
    "\n",
    "# Reshape rotated key per token to match the original shape\n",
    "k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "\n",
    "# Print the shape of the rotated key per token\n",
    "k_per_token_rotated.shape  # Output: torch.Size([17, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83f58c-8b01-42aa-9488-c935a3b531c7",
   "metadata": {},
   "source": [
    "### Implement self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0c685c16-d5d9-486a-838a-5176c079cc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "torch.Size([9, 128]) torch.Size([128, 9])\n"
     ]
    }
   ],
   "source": [
    "print(head_dim)\n",
    "print(q_per_token_rotated.shape, k_per_token_rotated.T.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dbf1ce45-06b9-4f4d-b2dc-cdc50910c377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 9])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate query-key dot products per token\n",
    "qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (head_dim) ** 0.5\n",
    "\n",
    "# Print the shape of the resulting tensor representing query-key dot products per token\n",
    "qk_per_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8cbb3428-e3ab-4b6e-927c-19d543ef7a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7595, 1.5931, 1.2744, 1.2071, 1.4154, 2.1142, 1.8662, 1.5409, 1.4746],\n",
       "        [9.5143, 6.2809, 4.6963, 3.5233, 3.4170, 3.8271, 3.9491, 4.2899, 3.9911],\n",
       "        [9.9251, 8.2805, 6.6103, 3.8961, 1.9111, 1.9750, 1.3963, 3.1151, 4.2871],\n",
       "        [9.5106, 9.0878, 8.4867, 6.4063, 3.8344, 2.7558, 0.3039, 1.2163, 3.0849],\n",
       "        [8.9338, 8.8067, 9.1704, 8.2739, 6.4438, 4.5944, 1.0971, 0.1569, 1.6121],\n",
       "        [8.1783, 7.2689, 7.3711, 7.2926, 6.7289, 7.2121, 4.6933, 2.5593, 2.3273],\n",
       "        [9.1369, 8.5826, 8.6075, 8.8851, 8.7706, 8.6583, 6.7642, 2.7599, 0.9424],\n",
       "        [8.1441, 7.1941, 7.0830, 7.3355, 7.6451, 8.8202, 7.7363, 5.4743, 2.5069],\n",
       "        [8.4862, 7.1415, 7.0239, 7.1970, 7.4780, 8.5692, 8.6379, 6.8205, 6.5807]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result is a 17 x 17 matrix with the attention scores of every word in the input sequence with every other word (including itself)\n",
    "qk_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b500998f-ed00-4124-89c2-9c67829381fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask tensor filled with negative infinity values\n",
    "mask = torch.full((len(tokens), len(tokens)), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "# Keep upper triangular part of the mask tensor as negative infinity, set the rest to zeros\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "# Print the resulting mask tensor\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "28082fae-1951-41b9-81eb-7051ad8a5b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9609, 0.0378, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8125, 0.1572, 0.0295, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4863, 0.3184, 0.1748, 0.0217, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2676, 0.2354, 0.3379, 0.1377, 0.0221, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3477, 0.1396, 0.1553, 0.1436, 0.0815, 0.1318, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2305, 0.1318, 0.1357, 0.1787, 0.1592, 0.1426, 0.0215, 0.0000, 0.0000],\n",
       "        [0.1826, 0.0703, 0.0630, 0.0811, 0.1108, 0.3594, 0.1211, 0.0126, 0.0000],\n",
       "        [0.2119, 0.0552, 0.0491, 0.0583, 0.0771, 0.2305, 0.2461, 0.0400, 0.0315]],\n",
       "       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the mask to the query-key dot products per token\n",
    "qk_per_token_after_masking = qk_per_token + mask\n",
    "\n",
    "# Apply softmax along the second dimension after masking\n",
    "qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "\n",
    "qk_per_token_after_masking_after_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6675794-e3a4-403e-bb1f-e015417c8b47",
   "metadata": {},
   "source": [
    "### Apply the attention scores to the value matrix\n",
    "* So far we've produced an attention score matrix that captures only the relationships between tokens (how much each token should focus on the others) but does not carry the original token embedding or positional information forward.  \n",
    "* The next step applies these attention scores to the values (which still retain both the word embeddings and positional encodings), re-incorporating these elements back into the representation, resulting in a contextually aware embedding for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829891e8-7d2a-483d-9ddb-ac04d0ebb086",
   "metadata": {},
   "source": [
    "For the value matrix, which marks the end of the self-attention part, similar to keys, value weights are also shared across every 4 attention heads to save computation. As a result, the shape of the value weight matrix is [8x128x4096]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f048900c-c288-4614-9c93-19423616ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4096])\n",
      "v_layer0.shape[0] =  1024\n",
      "n_kv_heads =  8\n",
      "v_layer0.shape[0] // n_kv_heads =  128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 4096])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Retrieve the value weight for the first layer of attention\n",
    "v_layer0 = model[\"layers.0.attention.wv.weight\"]\n",
    "print(v_layer0.shape)\n",
    "\n",
    "print(\"v_layer0.shape[0] = \", v_layer0.shape[0])\n",
    "print(\"n_kv_heads = \", n_kv_heads)\n",
    "print(\"v_layer0.shape[0] // n_kv_heads = \", v_layer0.shape[0] // n_kv_heads)\n",
    "\n",
    "# Reshape value weight for the first layer of attention to separate heads\n",
    "v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)\n",
    "\n",
    "# Print the shape of the reshaped value weight tensor\n",
    "v_layer0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8de5e-debe-496b-b2a1-9163587ff091",
   "metadata": {},
   "source": [
    "Obtain value weight matrix for first layer and first head..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "135031c4-36a3-4fa2-9ce3-d9ac844f46b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4096])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the value weight for the first head of the first layer of attention\n",
    "v_layer0_head0 = v_layer0[0]\n",
    "\n",
    "# Print the shape of the extracted value weight tensor for the first head\n",
    "v_layer0_head0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfd5cf-6135-4096-a9ff-68f870df3170",
   "metadata": {},
   "source": [
    "Using the value weights, we compute the attention values for each token, resulting in a matrix of size [17x128]. Here, 17 denotes the number of tokens in the prompt, and 128 indicates the dimension of the value vector for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "797893ad-b5b4-4c75-b7b1-2485f866b62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 128])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate value per token by matrix multiplication\n",
    "v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)\n",
    "\n",
    "# Print the shape of the resulting tensor representing values per token\n",
    "v_per_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce6d3f-520a-44e9-b645-69e6bc60d231",
   "metadata": {},
   "source": [
    "Obtain the resulting attention matri..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "82e3f537-c041-4929-a89b-3eb384a4c403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 128])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate QKV attention by matrix multiplication\n",
    "qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "qkv_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53814cc4-58da-49ee-b642-94f10fd0761b",
   "metadata": {},
   "source": [
    "***We now have the attention values for the first layer and first head or in other words self attention.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c69b2e-111b-4b20-8dc5-eaff36bcb206",
   "metadata": {},
   "source": [
    "## Part 4: Rinse and repeat: implement multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61acef6-fd8d-4316-a98f-98aa06fcdaaa",
   "metadata": {},
   "source": [
    "Do all of that again in a loop, once for each head on the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d21d38c7-be2e-4fb8-939c-1a2b3b568dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store QKV attention for each head in a list\n",
    "qkv_attention_store = []\n",
    "\n",
    "# Iterate through each head\n",
    "for head in range(n_heads):\n",
    "    # Extract query, key, and value weights for the current head\n",
    "    q_layer0_head = q_layer0[head]\n",
    "    k_layer0_head = k_layer0[head//4]  # Key weights are shared across 4 heads\n",
    "    v_layer0_head = v_layer0[head//4]  # Value weights are shared across 4 heads\n",
    "    \n",
    "    # Calculate query per token by matrix multiplication\n",
    "    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)\n",
    "    \n",
    "    # Calculate key per token by matrix multiplication\n",
    "    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)\n",
    "    \n",
    "    # Calculate value per token by matrix multiplication\n",
    "    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)\n",
    "    \n",
    "    # Split query per token into pairs and rotate them\n",
    "    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)])\n",
    "    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "    \n",
    "    # Split key per token into pairs and rotate them\n",
    "    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)])\n",
    "    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "    \n",
    "    # Calculate query-key dot products per token\n",
    "    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
    "    \n",
    "    # Create a mask tensor filled with negative infinity values\n",
    "    mask = torch.full((len(tokens), len(tokens)), float(\"-inf\"), device=tokens.device)\n",
    "    # Set upper triangular part of the mask tensor to negative infinity\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    # Add the mask to the query-key dot products per token\n",
    "    qk_per_token_after_masking = qk_per_token + mask\n",
    "    \n",
    "    # Apply softmax along the second dimension after masking\n",
    "    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "    \n",
    "    # Calculate QKV attention by matrix multiplication\n",
    "    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "    \n",
    "    # Store QKV attention for the current head\n",
    "    qkv_attention_store.append(qkv_attention)\n",
    "\n",
    "# Print the number of QKV attentions stored\n",
    "len(qkv_attention_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "01f32a4b-435b-478b-a13f-f438fb505e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 128])\n"
     ]
    }
   ],
   "source": [
    "print(qkv_attention_store[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58023a53-7ebb-4f0e-8042-44a5d087d94b",
   "metadata": {},
   "source": [
    "Now that the QKV attention matrix for all 32 heads in the first layer is obtained, all attention scores will be merged into one large matrix of size [17x4096].."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f8d96a96-c239-4576-bd69-bde5ae470f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate QKV attentions from all heads along the last dimension\n",
    "stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "stacked_qkv_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28462107-c5d6-465c-9281-680e6c3595e4",
   "metadata": {},
   "source": [
    "Now multiply the weight matrix with the stacked QKV matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fd723bee-8df3-4b88-9dd3-430e120061d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the embedding delta by matrix multiplication with the output weight\n",
    "embedding_delta = torch.matmul(stacked_qkv_attention, model[\"layers.0.attention.wo.weight\"].T)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "embedding_delta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d722bf-46b2-438d-b59e-15c76271d190",
   "metadata": {},
   "source": [
    "This gives us the change in the embedding values after attention.  Now add these to the original token embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "895dede4-9e24-4a51-b17e-f8ebc6b079f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the embedding delta to the unnormalized token embeddings to get the final embeddings\n",
    "embedding_after_edit = token_embeddings_unnormalized + embedding_delta\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "embedding_after_edit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507349ff-5575-467f-a47c-290986170f9e",
   "metadata": {},
   "source": [
    "Normalise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "adea8f5b-f091-4907-ba87-ea4c831a46c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize edited embeddings using root mean square normalization and provided weights\n",
    "embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[\"layers.0.ffn_norm.weight\"])\n",
    "\n",
    "# Print the shape of resulting normalized embeddings\n",
    "embedding_after_edit_normalized.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575072d4-003c-46be-8c88-9ca53238005f",
   "metadata": {},
   "source": [
    "### Now implement the feed forward neural network with a SwiGLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8c3e2a88-5717-4c54-ab2e-56dc8d884db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve weights for feedforward layer\n",
    "w1 = model[\"layers.0.feed_forward.w1.weight\"]\n",
    "w2 = model[\"layers.0.feed_forward.w2.weight\"]\n",
    "w3 = model[\"layers.0.feed_forward.w3.weight\"]\n",
    "\n",
    "# Perform operations for feedforward layer\n",
    "output_after_feedforward = torch.matmul(\n",
    "    torch.functional.F.silu(\n",
    "        torch.matmul(embedding_after_edit_normalized, w1.T)\n",
    "    ) \n",
    "    * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T\n",
    ")\n",
    "\n",
    "# Print the shape of the resulting tensor after feedforward\n",
    "output_after_feedforward.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "affafd48-d788-4d42-94e9-31446047eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize final embedding with unnormalized token embeddings\n",
    "final_embedding = token_embeddings_unnormalized\n",
    "\n",
    "# Iterate through each layer\n",
    "for layer in range(n_layers):\n",
    "    # Initialize list to store QKV attentions for each head\n",
    "    qkv_attention_store = []\n",
    "    \n",
    "    # Normalize the final embedding using root mean square normalization and weights from the current layer\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model[f\"layers.{layer}.attention_norm.weight\"])\n",
    "    \n",
    "    # Retrieve query, key, value, and output weights for the attention mechanism of the current layer\n",
    "    q_layer = model[f\"layers.{layer}.attention.wq.weight\"]\n",
    "    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)\n",
    "    k_layer = model[f\"layers.{layer}.attention.wk.weight\"]\n",
    "    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
    "    v_layer = model[f\"layers.{layer}.attention.wv.weight\"]\n",
    "    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
    "    w_layer = model[f\"layers.{layer}.attention.wo.weight\"]\n",
    "    \n",
    "    # Iterate through each head\n",
    "    for head in range(n_heads):\n",
    "        # Extract query, key, and value weights for the current head\n",
    "        q_layer_head = q_layer[head]\n",
    "        k_layer_head = k_layer[head//4]  # Key weights are shared across 4 heads\n",
    "        v_layer_head = v_layer[head//4]  # Value weights are shared across 4 heads\n",
    "        \n",
    "        # Calculate query per token by matrix multiplication\n",
    "        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)\n",
    "        \n",
    "        # Calculate key per token by matrix multiplication\n",
    "        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)\n",
    "        \n",
    "        # Calculate value per token by matrix multiplication\n",
    "        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)\n",
    "        \n",
    "        # Split query per token into pairs and rotate them\n",
    "        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)\n",
    "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "        \n",
    "        # Split key per token into pairs and rotate them\n",
    "        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "        \n",
    "        # Calculate query-key dot products per token\n",
    "        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
    "        \n",
    "        # Create a mask tensor filled with negative infinity values\n",
    "        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(\"-inf\"))\n",
    "        # Set upper triangular part of the mask tensor to negative infinity\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        # Add the mask to the query-key dot products per token\n",
    "        qk_per_token_after_masking = qk_per_token + mask\n",
    "        \n",
    "        # Apply softmax along the second dimension after masking\n",
    "        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "        \n",
    "        # Calculate QKV attention by matrix multiplication\n",
    "        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "        \n",
    "        # Store QKV attention for the current head\n",
    "        qkv_attention_store.append(qkv_attention)\n",
    "    \n",
    "    # Concatenate QKV attentions from all heads along the last dimension\n",
    "    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
    "    \n",
    "    # Calculate embedding delta by matrix multiplication with the output weight\n",
    "    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)\n",
    "    \n",
    "    # Add the embedding delta to the current embedding to get the edited embedding\n",
    "    embedding_after_edit = final_embedding + embedding_delta\n",
    "    \n",
    "    # Normalize the edited embedding using root mean square normalization and weights from the current layer\n",
    "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f\"layers.{layer}.ffn_norm.weight\"])\n",
    "    \n",
    "    # Retrieve weights for the feedforward layer\n",
    "    w1 = model[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
    "    w2 = model[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
    "    w3 = model[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
    "    \n",
    "    # Perform operations for the feedforward layer\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "    \n",
    "    # Update the final embedding with the edited embedding plus the output from the feedforward layer\n",
    "    final_embedding = embedding_after_edit + output_after_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad079e41-ddaf-4b72-9584-1fc329bbdafd",
   "metadata": {},
   "source": [
    "## Part 5: put it all together for all 32 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc383e6-bc25-4ab1-b2d2-6caf3fa596ba",
   "metadata": {},
   "source": [
    "### Generate the model's output\n",
    "The final embedding shouild represent the model's guess for the next token.  \n",
    "It has the same shape as the token embeddings [17x40896]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "00e80554-683d-49b6-9978-f340d9d13528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4096])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the final embedding using root mean square normalization and provided weights\n",
    "final_embedding = rms_norm(final_embedding, model[\"norm.weight\"])\n",
    "\n",
    "# Print the shape of the resulting normalized final embedding\n",
    "final_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fee6ca7d-e131-4748-a339-2b4affb15477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256, 4096])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the output weight tensor\n",
    "model[\"output.weight\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7225b1-9dc2-458f-ad48-0a0dc12a6bf5",
   "metadata": {},
   "source": [
    "To predict the next value, use the embedding of the last token..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4afbe4ca-96d3-4567-b17d-d8a0d8f6af86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate logits by matrix multiplication between the final embedding and the transpose of the output weight tensor\n",
    "logits = torch.matmul(final_embedding[-1], model[\"output.weight\"].T)\n",
    "\n",
    "# Print the shape of the resulting logits tensor\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "eb562b52-95db-4882-a964-7c6460ae8811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15763)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the index of the maximum value along the last dimension to determine the next token\n",
    "next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Output the index of the next token\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1db32376-7b4d-4dd5-a0d9-f83df2fff3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' appreciate'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the index of the next token using the tokenizer\n",
    "tokenizer.decode([next_token.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "bb36eee8-0ed9-4287-aff1-6e86a804da45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15763)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b9be3c66-fbb2-439a-a15d-c6f61f892381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15763\n"
     ]
    }
   ],
   "source": [
    "print(next_token.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c791e112-ff5c-451e-875e-3a7b0d651133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1041])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0aae42-19db-4108-80ce-e739fc46fd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
